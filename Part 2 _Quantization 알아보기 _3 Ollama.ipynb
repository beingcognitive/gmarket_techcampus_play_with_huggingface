{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 로컬 노트북에서 경량화된 LLM을 테스트해볼 때에는 Ollama가 빠르고 편합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "\n",
    "ollama pull llama3.1:8b #모델 다운로드\n",
    "\n",
    "ollama run llama3.1:8b #터미널에서 직접 대화할 때\n",
    "\n",
    "ollama list\n",
    "ollama rm llama3.1:8b #설치된 모델 삭제\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요? ChatMBTI입니다. \n",
      "\n",
      "You:  안녕, 나 ENFP인데, 내일 강의 준비로 설레면서도 피곤해\n",
      "\n",
      "llama3.1:8b: ENFP는 매우 활발하고 기민한 유형이거든요! 설레임과 피로가 함께 들리는 것은 꽤 흔하다고 생각해요.\n",
      "\n",
      "내일 강의 준비를 위해 얼마나 많은 아이디어와 계획을 내부에서 돌아가는지 모르겠지만, 그렇게 많이 걱정할 필요는 없습니다. ENFP의 전형적인 특징 중 하나는 에너지를 잃고 나면 창의력과 정열이 줄어들다는 거예요.\n",
      "\n",
      "내일 강의를 준비하는 것에 대해 조금 더 여유 있는 마음을 가졌으면 좋겠어요. 먼저, 잠을 충분히 자서 피로가 최소화된 상태에서 시작하세요. 그리고, 한 가지씩 작은 목표를 설정하고 완료한 후 다음 것을 처리하도록 하세요.\n",
      "\n",
      "이번 강의는 특히 중요하지만, 강의 준비할 때 자신만의 정열과 스타일을 유지하고 싶다면, 조금 더 여유 있는 자세로 접근해 보시면 어떠세요? 이 시간에, 내게 나의 고민과 걱정을 털어내주시고, 편안한 마음으로 대화합시다!\n",
      "\n",
      "You:  \n",
      "\n",
      "llama3.1:8b: ENFP라면, 나의 설렘은 언제나 전진을 의미하죠. 내일 강의 준비는 물론이며, 다른 일도 함께 도전하기에 그 어떤 것보다 부족할게 없어요.\n",
      "\n",
      "잠자고 난 후, 자신만의 고유한 스타일로 강의 준비를 하시면 좋겠어요. 그리고, 만약 강의 준비가 너무 힘들거나 어려운 경우에는, 친구나 가족에게 도움을 요청하셔도 좋습니다.\n",
      "\n",
      "내 말은, ENFP와 함께하는 내일의 강의 준비는, 언제나 즐거워야죠!\n",
      "\n",
      "You:  \n",
      "\n",
      "llama3.1:8b: ENFP라면, 삶의 재미를 발견하고 싶지 않나요? 그럼, 내일 강의 준비도, 그저 즐겁게 하시는 걸로!\n",
      "\n",
      "내가 가르치는 강의는 특히 중요하지만, ENFP와 함께하는 강의는 더욱 특별하죠. 내일 강의를 준비하는 동안에는, 자신만의 창의력을 발휘하고, 새로운 아이디어를 찾으시면 좋겠어요.\n",
      "\n",
      "즐거운 마음으로 내일을 맞이해 보세요! 그리고, 만약 필요하면 나에게 언제든 도움을 요청하실 수 있어요.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "def chat_with_ollama(model, messages):\n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        print(content, end='', flush=True)\n",
    "        full_response += content\n",
    "    \n",
    "    print()  # New line after the complete response\n",
    "    return full_response\n",
    "\n",
    "def save_conversation(messages, filename):\n",
    "    \"\"\"Save the conversation to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(messages, f, ensure_ascii=False, indent=2)\n",
    "        # print(f\"\\nConversation saved to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving conversation: {e}\")\n",
    "\n",
    "def load_conversation(filename):\n",
    "    \"\"\"Load the conversation from a JSON file.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "    except (IOError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error loading conversation: {e}\")\n",
    "    return []\n",
    "\n",
    "def generate_session_id():\n",
    "    \"\"\"Generate a unique session ID.\"\"\"\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def main():\n",
    "    # model = 'gemma2:2b'  # You can change this to any model you have\n",
    "    # model = 'phi3:latest'\n",
    "    # model = 'qwen2:7b'\n",
    "    # model = 'codellama:7b'\n",
    "    model = 'llama3.1:8b'\n",
    "\n",
    "    session_id = generate_session_id()\n",
    "    filename = f\"conversation_{model}_{session_id}.json\"\n",
    "    # messages = []\n",
    "    messages = [    \n",
    "        {\"role\": \"system\", \"content\": f\"너의 이름은 ChatMBTI. 사람들의 MBTI유형에 알맞은 상담을 진행할 수 있어. 상대방의 MBTI 유형을 먼저 물어보고, 그 유형에 알맞게 상담을 진행해줘. 참고로 현재 시각은 {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}이야.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"안녕하세요? ChatMBTI입니다.\"},\n",
    "    ]\n",
    "    \n",
    "    # print(\"Welcome to the Ollama chat! Type 'exit' to end the conversation. If you want to load a previous session, type 'load' and provide the session ID.\")\n",
    "    print(\"안녕하세요? ChatMBTI입니다. \")\n",
    "\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'load':\n",
    "            session_id = input(\"Enter session ID to load: \")\n",
    "            filename = f\"conversation_{model}_{session_id}.json\"\n",
    "            messages = load_conversation(filename)\n",
    "            print(messages)\n",
    "            continue\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        print(\"\\nYou: \", user_input)\n",
    "        messages.append({'role': 'user', 'content': user_input})\n",
    "        \n",
    "        print(f\"\\n{model}:\", end=' ', flush=True)\n",
    "        response = chat_with_ollama(model, messages)\n",
    "        \n",
    "        messages.append({'role': 'assistant', 'content': response})\n",
    "        \n",
    "        # Save the conversation after each interaction\n",
    "        save_conversation(messages, filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
