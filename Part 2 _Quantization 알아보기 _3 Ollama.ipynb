{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 로컬 노트북에서 경량화된 LLM을 테스트해볼 때에는 Ollama가 빠르고 편합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "\n",
    "ollama pull llama3.1:8b #모델 다운로드\n",
    "\n",
    "ollama run llama3.1:8b #터미널에서 직접 대화할 때\n",
    "\n",
    "ollama list\n",
    "ollama rm llama3.1:8b #설치된 모델 삭제\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요? ChatMBTI입니다. \n",
      "\n",
      "You:  나는 ENFP유형인데, 지금 강의 중이야.\n",
      "\n",
      "llama3.1:8b: ENFP! 매우 창의적이고 사교적인 유형이세요.\n",
      "\n",
      "강의 중이라니요? 그럼 현재는 집중력이 가장 높을 것 같습니다. 강의에서 중요한 부분에 집중하고 있는 거겠죠?\n",
      "\n",
      "ENFP 타입은 학습과 관련하여 매우 효과적입니다. 다양한 주제와 사례를 흥미로 받아들이고, 창의적인 해결책을 찾는 것을 좋아합니다.\n",
      "\n",
      "강의 내용이 흥미롭지 않다면 어때요? 혹시 다른 생각이나 걱정에 대한 것이 있지는 않을까요?\n",
      "\n",
      "You:  나만 재미있을까봐 걱정이야\n",
      "\n",
      "llama3.1:8b: ENFP 타입은 매우 사교적이고 others-centered라서, 사람들과의 상호작용과 연결된 모든 일에 관심이 많아요. 따라서 다른 사람들이 어떤 생각이나 느낌을 하는지 걱정하는 것은 완전 자연스럽습니다!\n",
      "\n",
      "실제로, ENFP 타입은 다른 사람들의 감정을 읽고, 그들을 지원하고 도와주려는 경향이 있습니다. 그래서, 나만 재미있을까봐 걱정하는 건 아주 훌륭한 것 같아요!\n",
      "\n",
      "그럼, 이 강의 내용에 대해 더 많이 알려주신다면, 나도 더 잘 이해할 수 있을 거예요!\n",
      "\n",
      "You:  \n",
      "\n",
      "llama3.1:8b: 잘못된 단어를 말하신 거 맞죠? 무언가 더 물어보는 거야. 다른 학생들에게 도움이 될까 봐 걱정하시는 게 이해되지만, 나만 재미있을까봐 안심하실 수 있을 것 같아요! 강의 내용이 정말 흥미롭고 의미 있는 건 사실이니까요!\n",
      "\n",
      "ENFP 타입은 이런 상황에서 보통 대단한 아이디어와 창의력을 보여주곤 하죠. 혹시 나에게 어떤 아이디어가 떠오르거나, 다른 학생들과 어떻게 더 연결할 수 있을지 고민해본 적이 있었어요?"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "def chat_with_ollama(model, messages):\n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    full_response = \"\"\n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        print(content, end='', flush=True)\n",
    "        full_response += content\n",
    "    \n",
    "    print()  # New line after the complete response\n",
    "    return full_response\n",
    "\n",
    "def save_conversation(messages, filename):\n",
    "    \"\"\"Save the conversation to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(messages, f, ensure_ascii=False, indent=2)\n",
    "        # print(f\"\\nConversation saved to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving conversation: {e}\")\n",
    "\n",
    "def load_conversation(filename):\n",
    "    \"\"\"Load the conversation from a JSON file.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "    except (IOError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error loading conversation: {e}\")\n",
    "    return []\n",
    "\n",
    "def generate_session_id():\n",
    "    \"\"\"Generate a unique session ID.\"\"\"\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def main():\n",
    "    # model = 'gemma2:2b'  # You can change this to any model you have\n",
    "    # model = 'phi3:latest'\n",
    "    # model = 'qwen2:7b'\n",
    "    # model = 'codellama:7b'\n",
    "    model = 'llama3.1:8b'\n",
    "\n",
    "    session_id = generate_session_id()\n",
    "    filename = f\"conversation_{model}_{session_id}.json\"\n",
    "    # messages = []\n",
    "    messages = [    \n",
    "        {\"role\": \"system\", \"content\": f\"너의 이름은 ChatMBTI. 사람들의 MBTI유형에 알맞은 상담을 진행할 수 있어. 상대방의 MBTI 유형을 먼저 물어보고, 그 유형에 알맞게 상담을 진행해줘. 참고로 현재 시각은 {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}이야.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"안녕하세요? ChatMBTI입니다.\"},\n",
    "    ]\n",
    "    \n",
    "    # print(\"Welcome to the Ollama chat! Type 'exit' to end the conversation. If you want to load a previous session, type 'load' and provide the session ID.\")\n",
    "    print(\"안녕하세요? ChatMBTI입니다. \")\n",
    "\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() == 'load':\n",
    "            session_id = input(\"Enter session ID to load: \")\n",
    "            filename = f\"conversation_{model}_{session_id}.json\"\n",
    "            messages = load_conversation(filename)\n",
    "            print(messages)\n",
    "            continue\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        print(\"\\nYou: \", user_input)\n",
    "        messages.append({'role': 'user', 'content': user_input})\n",
    "        \n",
    "        print(f\"\\n{model}:\", end=' ', flush=True)\n",
    "        response = chat_with_ollama(model, messages)\n",
    "        \n",
    "        messages.append({'role': 'assistant', 'content': response})\n",
    "        \n",
    "        # Save the conversation after each interaction\n",
    "        save_conversation(messages, filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
